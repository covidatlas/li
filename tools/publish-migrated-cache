#! /usr/bin/env node

const aws = require('aws-sdk')
const fs = require('fs')
const { join } = require('path')
const glob = require('glob').sync
const datetime = require('../src/shared/datetime/index.js')
const globJoin = require('../src/shared/utils/glob-join.js')
const crypto = require('crypto')

aws.config.setPromisesDependency(null)
const s3 = new aws.S3()

// ↓ Set this to true to dry-run / debug, or false if you actually want to mutate S3!
const testing = false

console.time('Ran in')

// Load up cached file list
const cacheDir = join(process.cwd(), 'migrated-cache')
const pattern = globJoin(cacheDir, '**')
let migratedCacheFiles = glob(pattern, { nodir: true })
console.log(`Found ${migratedCacheFiles.length} cache files`)

// Filter by latest day (if specified)
if (process.env.LATEST) {
  let latest = datetime.today.at('America/Los_Angeles')
  latest = datetime.getYYYYMMDD(latest)
  migratedCacheFiles = migratedCacheFiles.filter(file => {
    const path = file.replace(cacheDir, '').substr(1)
    return path.split('/')[1] === latest
  })
  console.log(`Using ${migratedCacheFiles.length} latest cache files`)
}

// Default to staging
process.env.NODE_ENV = process.env.NODE_ENV ? process.env.NODE_ENV : 'staging'
const Bucket = `li-cache-${process.env.NODE_ENV}`

// Checks to see if file is already there
async function head (params) {
  try {
    const head = s3.headObject(params)
    return await head.promise()
  }
  catch (err) {
    if (err.name === 'NotFound') return false
    else throw Error(err)
  }
}

// Puts file on S3
async function put (params) {
  const put = s3.putObject(params)
  return await put.promise()
}

// Destroys file on S3
async function del (params) {
  const deleteObj = s3.deleteObject(params)
  return await deleteObj.promise()
}

// Checksum file bodies
function hash (data) {
  return crypto
    .createHash('md5')
    .update(data)
    .digest('hex')
}

let counter = 0
let ignored = 0
let uploaded = 0
let destroyed = 0

if (!testing) {
  console.log(`Live cache mutation begins in 3 seconds!`)
  setTimeout(() => {
    go()
  }, 3000)
}
else go()
async function go () {
  for (const file of migratedCacheFiles) {
    counter++
    const Key = file.replace(cacheDir, '').substr(1)

    if (!testing) {
      // Look for junk to clear
      const deletes = [ 'arcgis', 'arcorgid', 'tempindex', 'tmpindex', 'tmpcsrf' ]
      const parts = Key.split('000z-')
      const destroy = deletes.some(s => parts[1].toLowerCase().startsWith(s))
      if (destroy) {
        await del({ Bucket, Key })
        destroyed++
        console.log(`Destroyed unnecessary file: ${Key}`)
        continue
      }

      // Check to see if it already exists
      const exists = await head({ Bucket, Key })
      if (exists) ignored++

      // Upload it
      else {
        const Body = fs.readFileSync(file)
        const params = {
          ACL: 'public-read',
          Bucket,
          Key,
          Body
        }

        const result = await put(params)
        const { ETag } = result
        const checksum = hash(Body)
        if (ETag.replace(/"/g, '') === checksum) {
          console.log(`Uploaded new file: ${Key}`)
        }
        // S3 chunk checksumming may occur if a file is extremely large, it's probably no biggie
        else console.warn(`⚠️  File uploaded, but checksums do not match: ${Key}`)
        uploaded++
      }
    }
  }

  console.log(`Looked at ... ${counter} files`)
  console.log(`Ignored ..... ${ignored} files`)
  console.log(`Uploaded .... ${uploaded} files`)
  console.log(`Destroyed ... ${destroyed} files`)
  console.timeEnd('Ran in')
}
